Our goal is to understand the theory of real functions in one variable. Specifically, we will deal with functions, limits, sequences, convergence, continuity, differentiation, and integration. The same ideas, concepts, and techniques are used to study more complicated mathematics.

We will primarily focus on the idea of \textbf{convergence}. Many computational techniques and algorithms rely on iteration---successive approximations getting closer to an actual solution. In order for those algorithms to work, they need to converge towards an actual solution.

To motivate our quest to learn about convergence, let's look at some classic iterative methods.

\begin{exbox}{Newton's Method}{}
    Given $c > 0$, suppose we want to calculate $\sqrt{c}$. Start with some initial guess $x_1 > 0$.
    \begin{alignat*}{2}
        & \text{Let}\quad & x_2 &\coloneq \frac{1}{2} \left( x_1 + \frac{c}{x_1} \right) \\
        & \text{Let}\quad & x_3 &\coloneq \frac{1}{2} \left( x_2 + \frac{c}{x_2} \right) \\
        && &\vdots \\
        & \text{Let}\quad & x_{n+1} &\coloneq \frac{1}{2} \left( x_n + \frac{c}{x_n} \right)
    \end{alignat*}
    We find that $\lim_{n\to\infty} x_n = \sqrt{c}$.
\end{exbox}

Does this method work for all $c > 0$ and $x_1 > 0$? Assuming $\lim_{n\to\infty} x_n = x$ converges, then:
\begin{alignat*}{2}
    && x_{n+1} &= \frac{1}{2} \left( x_n + \frac{c}{x_n} \right) \\
    &\implies \quad &x &= \frac{1}{2} \left( x + \frac{c}{x} \right) \\
    &\implies &2x &= x + \frac{c}{x} \\
    &\implies &x &= \frac{c}{x} \\
    &\implies &x^2 &= c \\
    &\implies &x &= \sqrt{c}
\end{alignat*}

The above calculation only makes sense if we know the sequence converges. Consider the sequence $x_{n+1} = 6 - x_n$ where $x_1 = 4$. Then:
\[ x_1 = 4,\quad x_2 = 2,\quad x_3 = 4,\quad x_4 = 2,\quad \ldots \]

% TODO: add reference for monotone convergence theorem
% TODO: explain what a bounded monotone sequence even is

Since this sequence does not converge, there is no limit when $n \to \infty$! In chapter 14, we will cover the Monotone Convergence Theorem, which states that any bounded monotone sequence converges.

\begin{exbox}{Monotone Convergence Theorem}{}
    Suppose that $c > 0$ and $x_1 > 0$. Then, for $n \geq 2$, the sequence $x_{n+1} = \frac{1}{2} \left( x-N + \frac{c}{x_n} \right)$ is:
    \begin{itemize}
        \item \textbf{bounded below} because $x_n > \sqrt{c}$ when $n \geq 2$, and
        \item \textbf{decreasing} because $x_n+1 < x_n$ for $n \geq 2$.
    \end{itemize}
    Therefore, $x_n$ converges by the Monotone Convergence Theorem, and $\lim_{n\to\infty} x_n = \sqrt{c}$.
\end{exbox}

Let's look at a more complicated iterative method.

\begin{exbox}{Picard's Method}{}
    Suppose we had to solve $y\prime = f(x,y)$ where $y(x_0) = y_0$ (i.e. find a function $y$ that satisfies our two conditions). As it turns out, we can use an iterated method to solve this as well.
    \begin{itemize}
        \item Start with an initial guess $y_1(x)$
        \item Define $\displaystyle y_{n+1}(x) \coloneq y_0 + \int_{x_0}^{x} f(t, y_n)\ dt$.
    \end{itemize}
    Provided that $f$ and $y_0$ are ``well-behaving'', then the sequence of functions $y_n(x)$ converges to the solution $y(x)$.
\end{exbox}

The idea that an infinite sequence of functions can converge suggests some notion of ``distance'' between functions. We can use a number of metrics for distance, some possibilities including:
\begin{itemize}
    \item \( \displaystyle \int_a^b \abs{f(x) - g(x)}\ dx \quad \) (total area between the two functions)
    \item \( \displaystyle \sup \left\{ x : x = \abs{f(x) - g(x)} \right\} \quad \) (max possible ``vertical'' distance between the two curves)
\end{itemize}
